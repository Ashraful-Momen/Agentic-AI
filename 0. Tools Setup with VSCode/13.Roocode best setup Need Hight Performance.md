Understood.
You are:

**• Using ONLY Cloud LLM APIs for reasoning** → (Minimax M2 API, Z.ai GLM 4.6 API)
**• Using ALL embedding + vector DB locally** → (Ollama + Qdrant)
**• Using RooCode only as a VS Code extension**
**• Need: the FASTEST + MOST STABLE + LARGEST codebase indexing (50k–200k chunks)**
**• Want the best RooCode extension configuration for performance**

Below is the **best possible real-world production setup** for 2025 with your exact environment.

---

# 1) The Correct Architecture (Your Setup, Optimized)

```
VS Code
└── RooCode Extension
      ├── Uses Cloud LLM API (Minimax M2, GLM 4.6)
      ├── Sends embedding jobs → Local Indexer
      └── Sends vector queries → Local Qdrant

Local Machine
├── Ollama (bge-m3 embeddings)
├── Qdrant (vector DB)
└── Indexer (Python or Node, not RooCode)
```

**IMPORTANT:**
RooCode **cannot** handle huge indexing alone (20k–100k+ blocks).
You must **offload all indexing to a custom local indexer**.
Then RooCode will only *query* the index, not build it.

This guarantees 10× faster performance and supports enormous codebases.

---

# 2) You Need Three Things (I provide all)

## A) **High-performance Qdrant configuration**

## B) **High-performance embedding/indexing pipeline** (Ollama → Qdrant)

## C) **High-performance RooCode config for VS Code**

I’m giving you all three below.

---

# 3) High-Performance Qdrant Setup (local)

Create Qdrant collection:

```bash
curl -X PUT "http://localhost:6333/collections/codebase" \
  -H "Content-Type: application/json" \
  -d '{
    "vectors": {
      "size": 1024,
      "distance": "Cosine"
    },
    "hnsw_config": {
      "m": 32,
      "ef_construct": 256
    },
    "optimizers_config": {
      "default_segment_number": 8
    },
    "quantization_config": {
      "scalar": {
        "type": "int8"
      }
    }
  }'
```

This is optimized for:

**• millions of vectors
• very long code chunk embedding
• high query accuracy
• low RAM usage
• high write throughput**

---

# 4) High-Performance Embedding Model (Ollama)

Use **bge-m3** (best long-context embed model):

```bash
ollama pull bge-m3
```

Local embed → **65k token window**, perfect for large code files.

---

# 5) The **Correct Chunk Size** for Large Codebase Indexing

For HUGE repos:

**maxChunkSize = 48,000 characters**
**chunkOverlap = 600–1,000 characters**

Reason:

* More files combined → fewer vectors
* Qdrant search becomes more accurate
* Indexing time reduces drastically
* RooCode context-retrieval becomes higher quality

---

# 6) Ultra-Fast Indexer (the thing RooCode CANNOT do alone)

Save this as:

`indexer.py`

```python
import os
import json
import hashlib
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import requests
from qdrant_client import QdrantClient

OLLAMA = "http://localhost:11434"
QDRANT = QdrantClient(url="http://localhost:6333")
COLLECTION = "codebase"
CHUNK = 48000
OVERLAP = 800
WORKERS = 8
BATCH = 256

def chunk_text(text):
    i=0
    L=len(text)
    while i < L:
        end = min(L, i+CHUNK)
        yield text[i:end]
        i = end - OVERLAP

def embed_batch(items):
    texts = [t for _,t,_ in items]
    r = requests.post(f"{OLLAMA}/api/embed", json={"model":"bge-m3","input":texts})
    r.raise_for_status()
    embs = r.json().get("embeddings")
    return embs

def process_file(path):
    t = path.read_text(errors="ignore")
    chunks = list(chunk_text(t))
    out=[]
    for i,c in enumerate(chunks):
        k = hashlib.sha1(f"{path}:{i}".encode()).hexdigest()
        out.append((k,c,{"path":str(path),"chunk":i}))
    return out

def index_repo(root):
    root = Path(root)
    files = [
        p for p in root.rglob("*")
        if p.is_file() and not any(x in p.parts for x in ["node_modules","vendor",".git","dist","build",".next"])
    ]

    print(f"Files: {len(files)}")

    all_chunks=[]
    with ThreadPoolExecutor(max_workers=WORKERS) as ex:
        for result in ex.map(process_file, files):
            all_chunks.extend(result)

    print(f"Total chunks: {len(all_chunks)}")

    for i in range(0, len(all_chunks), BATCH):
        batch = all_chunks[i:i+BATCH]
        embeddings = embed_batch(batch)
        points = []
        for (id_,_,meta), emb in zip(batch, embeddings):
            points.append({"id":id_, "vector":emb, "payload":meta})
        QDRANT.upsert(collection_name=COLLECTION, points=points)
        print("Uploaded", i)

if __name__=="__main__":
    import sys
    index_repo(sys.argv[1])
```

Run:

```bash
python indexer.py /your/project/root
```

**Do NOT let RooCode index by itself anymore.
Let RooCode only query Qdrant.**

---

# 7) Best RooCode Configuration (VS Code Extension)

Create:

`.roocode/config.json`

```json
{
  "indexing": {
    "embeddingModel": "local",
    "maxChunkSize": 48000,
    "chunkOverlap": 800,
    "parallelWorkers": 8,
    "enableLocalIndexing": false
  },
  "vectorDb": {
    "provider": "qdrant",
    "url": "http://localhost:6333",
    "collection": "codebase"
  },
  "llm": {
    "provider": "cloud",
    "models": {
      "default": "glm-4.6",
      "analysis": "minimax-m2"
    },
    "temperature": 0.1,
    "context": 200000
  }
}
```

Key settings:

### `"enableLocalIndexing": false`

This prevents RooCode from trying to index the repo itself
→ avoiding timeouts, crashes, and freezes.

### `"embeddingModel": "local"`

Means RooCode queries Qdrant, not embed anything.

---

# 8) Summary of the **Best Possible Setup**

| Layer              | Technology                     | Why                               |
| ------------------ | ------------------------------ | --------------------------------- |
| **Embeddings**     | bge-m3 (Ollama)                | Fast, long-context, local, stable |
| **Vector DB**      | Qdrant with int8 quantization  | Massive scale, fast recall        |
| **Indexer**        | External Python script         | 10× faster, avoids RooCode limits |
| **LLM Reasoning**  | Minimax + GLM 4.6 API          | Best code reasoning budget        |
| **RooCode Config** | Large context + external index | No crashes, best retrieval        |

This is the maximum-performance setup achievable today for VS Code + RooCode + local embeddings + Qdrant.

---

