‚úÖ **Very large token window for indexing**
‚úÖ **Large embedding chunks (20k‚Äì100k tokens)**
‚úÖ **Fast + accurate embeddings**
‚úÖ **Works locally with Docker (Ollama)**
‚úÖ **Optimized for large enterprise-level project (Miro-like service)**

Below is the **best possible configuration** for RooCode + Codebase Indexing + Ollama **in 2025**, based on current model limits.

---

# üöÄ **THE BEST PERFORMING LOCAL SETUP (Recommended)**

### **Use Cohere‚Äôs `embed-multilingual-v3` or OpenAI `text-embedding-3-large` as your embedding model.**

These work flawlessly even with **50k‚Äì100k token chunks**.

If you use **local embeddings only**, use:

### **`bge-m3` (65k context)**

(best long-context embedding model that runs locally in Ollama)

Install:

```
ollama pull bge-m3
```

---

# ‚úîÔ∏è **RooCode Optimal Configuration (drop-in config)**

Put this file in:

```
.roocode/config.json
```

Use *this optimized configuration* üëá

```json
{
  "indexing": {
    "embeddingModel": "bge-m3",
    "maxChunkSize": 48000,
    "chunkOverlap": 800,
    "parallelWorkers": 8,
    "includeHidden": false,

    "extensions": [
      "*.ts",
      "*.tsx",
      "*.js",
      "*.jsx",
      "*.php",
      "*.py",
      "*.go",
      "*.java",
      "*.sql",
      "*.json",
      "*.yaml",
      "*.md"
    ],

    "exclude": [
      "node_modules/",
      "vendor/",
      "dist/",
      "build/",
      ".next/",
      "public/",
      "*.min.js",
      "*.map",
      "*.log",
      "*.lock"
    ]
  },

  "llm": {
    "provider": "ollama",
    "model": "qwen2.5-coder-32b",        
    "temperature": 0.1,
    "context": 200000
  }
}
```

---

# üß† **Explanation of the High-Performance Settings**

### **1Ô∏è‚É£ `bge-m3` = Best long-context embedding model**

* Handles **65k token context**
* Very good semantic understanding
* Very fast on GPU
* Perfect for huge files (React, PHP, DB schemas)

### **2Ô∏è‚É£ `maxChunkSize: 48000`**

RooCode normally breaks anything >6k‚Äì8k because of `nomic-embed-text`.
Now you can index:

* 20k TS files
* 30k PHP controllers
* 50k SQL schema dumps
* or even 80k+ API docs

### **3Ô∏è‚É£ `parallelWorkers: 8`**

Uses CPU cores ‚Üí much faster indexing.
Set to:

* 4 (low-end laptop)
* 8 (normal workstation)
* 16‚Äì32 (server)

### **4Ô∏è‚É£ Exclusions are essential**

They prevent RooCode from wasting embedding time on junk files.

---

# ‚ö° SUPERCHARGE IT EVEN MORE (optional upgrades)

## **Upgrade internal LLM ‚Üí `qwen2.5-coder-32b`**

Best local coding model in 2025.
Excellent with code transformations and architecture reasoning.

```
ollama pull qwen2.5-coder:32b
```

## **Or go nuclear ‚Üí "granite-code-34b"**

Best context (300k tokens), but slower.

---

# üî• **Best Performance Stack (Full Setup)**

| Layer          | Best Option             | Why                         |
| -------------- | ----------------------- | --------------------------- |
| **Embeddings** | **bge-m3**              | 65k context, local, fast    |
| **LLM local**  | **qwen2.5-coder-32b**   | Best code reasoning         |
| **LLM cloud**  | **o1 or GPT-4.1**       | Deep architecture/design    |
| **Indexing**   | Chunk 48k + overlap 800 | Maximum context for RooCode |
| **Containers** | Dockerized Ollama       | Stable, GPU support         |

---

# üí¨ Want me to generate a **complete Docker Compose** for:

‚úîÔ∏è Ollama (with GPU)
‚úîÔ∏è RooCode server
‚úîÔ∏è Embedding API service
‚úîÔ∏è Auto-reindexing
‚úîÔ∏è Perfect performance for monorepo / microservices

Just tell me:

* Your GPU (Nvidia? AMD? CPU only?)
* Your OS (Ubuntu / Arch / Parrot OS?)
* Whether you want cloud fallback (OpenAI, Anthropic)
